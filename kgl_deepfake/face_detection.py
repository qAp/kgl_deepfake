# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_face_detection.ipynb (unless otherwise specified).

__all__ = ['DetectionPipeline', 'plt_frames_faces', 'plt_frames_faces_vert', 'get_frames_from_video', 'get_first_face',
           'VideoFaceList', 'extract_faces_from_frame']

# Cell
from IPython.display import HTML
import cv2
from fastai.core import *
from fastai.vision import *
from facenet_pytorch import MTCNN
import mmcv
from .nb_00 import *
from .facenet_pytorch.models.utils.detect_face import extract_face
import holoviews as hv

# Cell
class DetectionPipeline:
    def __init__(self, detector=None, nf=None, bs=60, fsz=None):
        if detector is None: detector = self.default_detector()
        self.detector = detector
        self.nf, self.bs, self.fsz = nf, bs, fsz

    def default_detector(self):
        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
        return MTCNN(device=device, image_size=256,
                     post_process=False, select_largest=False, keep_all=True)

    def get_sample(self, vlen, n_frames):
        if n_frames is None: return np.arange(0, vlen)
        else: return np.linspace(0, vlen - 1, n_frames).astype(int)

    def resize(self, frame, size):
        return frame.resize([int(d * size) for d in frame.size])

    def get_savepaths(self, filename, idxs, label=None, save_dir=None):
        if isinstance(filename, str): filename = Path(filename)
        if save_dir is None: save_dir = Path('./')
        if label is None: save_paths = [save_dir/f'{filename.stem}_{i:03d}.png' for i in idxs]
        else: save_paths = [save_dir/f'{filename.stem}_{i:03d}_{label}.png' for i in idxs]
        return [str(o) for o in save_paths]

    def equalize(self, frame):
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        frame[:,:,0] = clahe.apply(frame[:,:,0])
        return cv2.cvtColor(frame, cv2.COLOR_YCrCb2RGB)

    def __call__(self, filename, equalize=False, label=None, save_dir=None):
        assert Path(filename).exists()
        vcap = cv2.VideoCapture(str(filename))
        vlen = int(vcap.get(cv2.CAP_PROP_FRAME_COUNT))
        sample = self.get_sample(vlen, self.nf)
        iframes, faces, probs = [], [], []
        ib, fb = [], []
        for i in progress_bar(range(vlen)):
            _ = vcap.grab()
            if i in sample:
                success, f = vcap.retrieve()
                if not success: continue
                if equalize: f = self.equalize(f)
                else: f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)
                f = PIL.Image.fromarray(f)
                if self.fsz is not None: f = self.resize(f, self.fsz)
                ib.append(i); fb.append(f)
                if len(fb) % self.bs == 0 or i == sample[-1]:
                    savepaths = self.get_savepaths(filename, ib, label, save_dir) if save_dir else None
                    faceb, probb = self.detector(fb, return_prob=True, save_path=savepaths)
                    iframes.extend(ib); faces.extend(faceb); probs.extend(probb)
                    ib, fb = [], []
        vcap.release()
        return iframes, faces, probs

# Cell
def plt_frames_faces(iframes, faces, probs):
    ncols = len(iframes)
    nrows = max(len(face) for face in faces if face is not None)
    fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3*ncols, 3*nrows))
    if len(axs.shape) == 1: axs = axs[None,...]
    for axcol, iframe, faces_, probs_ in zip(axs.T, iframes, faces, probs):
        if faces_ is None:
            for i, ax in enumerate(axcol):
                if i==0: ax.set_title(f'frame {iframe}')
                ax.yaxis.set_ticklabels([]); ax.yaxis.set_ticks([])
                ax.xaxis.set_ticklabels([]); ax.xaxis.set_ticks([])
            continue

        for i, (ax, f_, p_) in enumerate(itertools.zip_longest(axcol, faces_, probs_)):
            if p_:
                ax.imshow(f_.permute(1, 2, 0).type(torch.uint8).numpy())
                ax.set_xlabel(f'prob = {p_:.4f}')
            if i==0: ax.set_title(f'frame {iframe}')
            ax.yaxis.set_ticklabels([]); ax.yaxis.set_ticks([])
            ax.xaxis.set_ticklabels([]); ax.xaxis.set_ticks([])

    return fig, axs

# Cell
def plt_frames_faces_vert(iframes, faces, probs):
    nrows = len(iframes)
    ncols = max(len(face) for face in faces if face is not None)
    fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(3*ncols, 3*nrows))
    if len(axs.shape) == 1: axs = axs[...,None]
    for axframe, iframe, faces_, probs_ in zip(axs, iframes, faces, probs):
        if faces_ is None:
            for i, ax in enumerate(axframe):
                if i==0: ax.set_ylabel(f'frame {iframe}')
                ax.yaxis.set_ticklabels([]); ax.yaxis.set_ticks([])
                ax.xaxis.set_ticklabels([]); ax.xaxis.set_ticks([])
            continue

        for i, (ax, f_, p_) in enumerate(itertools.zip_longest(axframe, faces_, probs_)):
            if p_:
                ax.imshow(f_.permute(1, 2, 0).type(torch.uint8).numpy())
                ax.set_xlabel(f'prob = {p_:.4f}')
            if i==0: ax.set_ylabel(f'frame {iframe}')
            ax.yaxis.set_ticklabels([]); ax.yaxis.set_ticks([])
            ax.xaxis.set_ticklabels([]); ax.xaxis.set_ticks([])

    return fig, axs

# Cell
def get_frames_from_video(fn, nf_max=30, spread=True):
    vcap = cv2.VideoCapture(str(fn))
    vlen = int(vcap.get(cv2.CAP_PROP_FRAME_COUNT))
    iframes, frames = [], []
    for i in range(vlen):
        success, frame = vcap.read()
        cond = np.random.rand()<(nf_max + 10)/vlen if spread else True
        if success and cond:
            iframes.append(i); frames.append(frame)
        if len(frames)==nf_max: break
    return iframes, frames

# Cell
def get_first_face(detector, fn, resize=.5, equalize=False):
    '''
    Returns the first detected face from a video
    '''
    assert Path(fn).exists()
    v_cap = cv2.VideoCapture(str(fn))
    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))
    iframe, face = None, None
    for i in range(v_len):
        _ = v_cap.grab()
        success, frame = v_cap.retrieve()
        if not success: continue
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YCrCb)
        if equalize:
            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
            frame[:,:,0] = clahe.apply(frame[:,:,0])
        frame = cv2.cvtColor(frame, cv2.COLOR_YCrCb2RGB)
        frame = PIL.Image.fromarray(frame)
        if resize is not None: frame = frame.resize([int(d * resize) for d in frame.size])
        face = detector(frame)
        if face is not None:
            iframe = i
            break
    v_cap.release()
    return iframe, face

# Cell
class VideoFaceList(ImageList):
    def __init__(self, *args, detector=None, device=None, resize=.5, equalize=True, **kwargs):
        super().__init__(*args, **kwargs)
        if device is None: device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        if detector is None: detector = MTCNN(device=device, post_process=False, select_largest=False, margin=0)
        self.detector, self.resize, self.equalize = detector, resize, equalize
        self.copy_new.extend(['detector', 'resize', 'equalize'])

    def get_face(self, fn:Path):
        iframe, face = get_first_face(self.detector, fn, self.resize, self.equalize)
        if iframe is None or face is None: raise Exception(f'No faces detected in {fn}')
        return iframe, face

    def open(self, fn:Path):
        iframe, face = self.get_face(fn)
        return Image(face / 255)

    @classmethod
    def from_df(cls, *args, df:DataFrame=None, path:PathOrStr=None, cols:str=None, **kwargs):
        fexists = df[cols].apply(lambda o: (Path(path)/o).exists())
        if sum(fexists) < len(df):
            print('WARNING: the following files cannot be found, leaving them out.')
            print(df[~fexists])
        df = df[fexists]
        return super().from_df(*args, df=df, path=path, cols=cols, **kwargs)

# Cell

def extract_faces_from_frame(frame, det, image_size=256, margin=20):
    '''
    Adpated from facenet_pytorch.
    '''
    if len(det.shape) == 1: return torch.Tensor([])
    im, box_im = PIL.Image.fromarray(frame), det[:,:4]
    faces_im = []
    for i, box in enumerate(box_im):
        face = extract_face(im, box, image_size, margin)
        faces_im.append(face)
    faces_im = torch.Tensor([]) if not faces_im else torch.stack(faces_im)
    return faces_im