{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a single face from a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp nb_01b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from kgl_deepfake.nb_00 import *\n",
    "from kgl_deepfake.nb_01 import *\n",
    "from IPython.display import HTML\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from facenet_pytorch import MTCNN\n",
    "from nbdev.export import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SOURCE = Path('../data/train_sample_videos/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>aagfhgtpmv.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vudstovrck.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>aapnvogymq.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>jdubbvfswz.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>abarnvbtwb.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>abofeumbvv.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>atvmxvwyns.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>abqwwspghj.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>qzimuostzz.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            fname label  split        original\n",
       "0  aagfhgtpmv.mp4  FAKE  train  vudstovrck.mp4\n",
       "1  aapnvogymq.mp4  FAKE  train  jdubbvfswz.mp4\n",
       "2  abarnvbtwb.mp4  REAL  train            None\n",
       "3  abofeumbvv.mp4  FAKE  train  atvmxvwyns.mp4\n",
       "4  abqwwspghj.mp4  FAKE  train  qzimuostzz.mp4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = get_files(SOURCE, extensions=['.json'])[0]\n",
    "annots = pd.read_json(f).T\n",
    "annots.reset_index(inplace=True)\n",
    "annots.rename({'index':'fname'}, axis=1, inplace=True)\n",
    "annots.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('../data/train_sample_videos/aagfhgtpmv.mp4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = SOURCE/annots.fname[0]\n",
    "fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the first detected face from a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "detector = MTCNN(device=device, post_process=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize = .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_first_face(detector, fn, resize=.5):\n",
    "    '''\n",
    "    Returns the first detected face from a video\n",
    "    '''\n",
    "    v_cap = cv2.VideoCapture(str(fn))\n",
    "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    iframe, face = None, None\n",
    "    for i in range(v_len):\n",
    "        _ = v_cap.grab()\n",
    "        success, frame = v_cap.retrieve()\n",
    "        if not success: continue\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = PIL.Image.fromarray(frame)\n",
    "        if resize is not None: frame = frame.resize([int(d * resize) for d in frame.size])\n",
    "        face = detector(frame)\n",
    "        if face is not None: \n",
    "            iframe = i\n",
    "            break\n",
    "        v_cap.release()\n",
    "    return iframe, face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Videos in which not a single face is detected by MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_has_face(fnames, detector):\n",
    "    if isinstance(fnames, (str, Path)): fnames = [fnames]\n",
    "    res = []\n",
    "    for i in progress_bar(range(len(fnames))):\n",
    "        iframe, face = get_first_face(detector, fnames[i])\n",
    "        res.append(True if iframe is not None else False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='400' class='' max='400', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [400/400 02:21<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fnames = [SOURCE/o for o in annots.fname]\n",
    "hasface = get_has_face(fnames, detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 20)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hasface), len([o for o in hasface if o == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots_noface = annots[~np.array(hasface)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annots_noface.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_noface = [SOURCE/o for o in annots_noface.fname]\n",
    "labels = [f'{o.fname} {o.label}' for i, o in annots_noface.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><div><p>adhsbajydo.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/adhsbajydo.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>agrmhtjdlk.mp4 REAL</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/agrmhtjdlk.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>andaxzscny.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/andaxzscny.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td></tr><tr><td><div><p>aorjvbyxhw.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/aorjvbyxhw.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>atvmxvwyns.mp4 REAL</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/atvmxvwyns.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>avvdgsennp.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/avvdgsennp.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td></tr><tr><td><div><p>axwgcsyphv.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/axwgcsyphv.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>bbvgxeczei.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/bbvgxeczei.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>bqkdbcqjvb.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/bqkdbcqjvb.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td></tr><tr><td><div><p>cdyakrxkia.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/cdyakrxkia.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>cwqlvzefpg.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/cwqlvzefpg.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>cyboodqqyr.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/cyboodqqyr.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td></tr><tr><td><div><p>cycacemkmt.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/cycacemkmt.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>czmqpxrqoh.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/czmqpxrqoh.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>dhoqofwoxa.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/dhoqofwoxa.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td></tr><tr><td><div><p>djvutyvaio.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/djvutyvaio.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>dkhlttuvmx.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/dkhlttuvmx.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>dqnyszdong.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/dqnyszdong.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td></tr><tr><td><div><p>eoewqcpbgt.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/eoewqcpbgt.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td><td><div><p>esyhwdfnxs.mp4 FAKE</p><br>\n",
       "    <video width=\"300\" height=\"250\" controls>\n",
       "    <source src=\"../data/train_sample_videos/esyhwdfnxs.mp4\" type=\"video/mp4\">\n",
       "    </video>\n",
       "    </div></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(html_vids(fnames_noface, titles=labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `ItemList` that returns the first face from a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class VideoFaceList(ImageList):\n",
    "    def __init__(self, *args, detector=None, resize=.5, device=None, **kwargs):\n",
    "        if device is None: device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        if detector is None: detector = MTCNN(device=device, post_process=False)\n",
    "        self.detector, self.resize = detector, resize\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def get_face(self, fn:Path):\n",
    "        iframe, face = get_first_face(self.detector, fn, self.resize)\n",
    "        if iframe is None or face is None: raise Exception(f'No faces detected in {fn}')\n",
    "        return iframe, face\n",
    "    \n",
    "    def open(self, fn:Path):\n",
    "        iframe, face = self.get_face(fn)\n",
    "        return Image(face / 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this, excluding those videos with no face detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380, 4)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annots[np.array(hasface)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "annots_hasface = annots[np.array(hasface)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = VideoFaceList.from_df(df=annots_hasface, path=SOURCE, cols='fname').split_by_rand_pct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 34.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ItemLists;\n",
       "\n",
       "Train: VideoFaceList (304 items)\n",
       "Image (3, 160, 160),Image (3, 160, 160),Image (3, 160, 160),Image (3, 160, 160),Image (3, 160, 160)\n",
       "Path: ../data/train_sample_videos;\n",
       "\n",
       "Valid: VideoFaceList (76 items)\n",
       "Image (3, 160, 160),Image (3, 160, 160),Image (3, 160, 160),Image (3, 160, 160),Image (3, 160, 160)\n",
       "Path: ../data/train_sample_videos;\n",
       "\n",
       "Test: None"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.1 s, sys: 2.1 s, total: 17.2 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data = src.label_from_df('label').databunch(bs=32, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 2.29 s, total: 17.1 s\n",
      "Wall time: 10.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xb, yb = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 160, 160]), torch.Size([32]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.3 s, sys: 2.17 s, total: 17.5 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "xb, yb = next(iter(data.valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3, 160, 160]), torch.Size([32]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_lookatdata.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted 01_face_recog.ipynb.\n",
      "Converted 01a_face_extraction.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted 01b_get_one_face.ipynb.\n",
      "Converted 02a_create_faceimage_dataset.ipynb.\n",
      "Converted 02bis_Create_Dataset-Copy1.ipynb.\n",
      "Converted 02bis_Create_Dataset.ipynb.\n",
      "Converted 03_mesonet.ipynb.\n",
      "Converted 04_Baseline_Classification-Copy1.ipynb.\n",
      "Converted 04_Baseline_Classification.ipynb.\n",
      "Converted 04_Classification-Copy1.ipynb.\n",
      "Converted 04_Classification.ipynb.\n",
      "Converted 04a_videolist.ipynb.\n",
      "Converted 05_Class_Imbalance-Copy1.ipynb.\n",
      "Converted 05_Class_Imbalance.ipynb.\n",
      "Converted 06_Focal_Loss.ipynb.\n",
      "Converted Untitled.ipynb.\n",
      "Converted test_submission.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
