{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp pytorch_retinaface.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mFDDB\u001b[m\u001b[m            \u001b[34m__pycache__\u001b[m\u001b[m     data_augment.py\r\n",
      "__init__.py     config.py       wider_face.py\r\n"
     ]
    }
   ],
   "source": [
    "! ls Pytorch_Retinaface/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# %load Pytorch_Retinaface/data/config.py\n",
    "# config.py\n",
    "\n",
    "cfg_mnet = {\n",
    "    'name': 'mobilenet0.25',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 32,\n",
    "    'ngpu': 1,\n",
    "    'epoch': 250,\n",
    "    'decay1': 190,\n",
    "    'decay2': 220,\n",
    "    'image_size': 640,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'stage1': 1, 'stage2': 2, 'stage3': 3},\n",
    "    'in_channel': 32,\n",
    "    'out_channel': 64\n",
    "}\n",
    "\n",
    "cfg_re50 = {\n",
    "    'name': 'Resnet50',\n",
    "    'min_sizes': [[16, 32], [64, 128], [256, 512]],\n",
    "    'steps': [8, 16, 32],\n",
    "    'variance': [0.1, 0.2],\n",
    "    'clip': False,\n",
    "    'loc_weight': 2.0,\n",
    "    'gpu_train': True,\n",
    "    'batch_size': 24,\n",
    "    'ngpu': 4,\n",
    "    'epoch': 100,\n",
    "    'decay1': 70,\n",
    "    'decay2': 90,\n",
    "    'image_size': 840,\n",
    "    'pretrain': True,\n",
    "    'return_layers': {'layer2': 1, 'layer3': 2, 'layer4': 3},\n",
    "    'in_channel': 256,\n",
    "    'out_channel': 256\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# %load Pytorch_Retinaface/data/data_augment.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from Pytorch_Retinaface.utils.box_utils import matrix_iof\n",
    "\n",
    "\n",
    "def _crop(image, boxes, labels, landm, img_dim):\n",
    "    height, width, _ = image.shape\n",
    "    pad_image_flag = True\n",
    "\n",
    "    for _ in range(250):\n",
    "        \"\"\"\n",
    "        if random.uniform(0, 1) <= 0.2:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = random.uniform(0.3, 1.0)\n",
    "        \"\"\"\n",
    "        PRE_SCALES = [0.3, 0.45, 0.6, 0.8, 1.0]\n",
    "        scale = random.choice(PRE_SCALES)\n",
    "        short_side = min(width, height)\n",
    "        w = int(scale * short_side)\n",
    "        h = w\n",
    "\n",
    "        if width == w:\n",
    "            l = 0\n",
    "        else:\n",
    "            l = random.randrange(width - w)\n",
    "        if height == h:\n",
    "            t = 0\n",
    "        else:\n",
    "            t = random.randrange(height - h)\n",
    "        roi = np.array((l, t, l + w, t + h))\n",
    "\n",
    "        value = matrix_iof(boxes, roi[np.newaxis])\n",
    "        flag = (value >= 1)\n",
    "        if not flag.any():\n",
    "            continue\n",
    "\n",
    "        centers = (boxes[:, :2] + boxes[:, 2:]) / 2\n",
    "        mask_a = np.logical_and(roi[:2] < centers, centers < roi[2:]).all(axis=1)\n",
    "        boxes_t = boxes[mask_a].copy()\n",
    "        labels_t = labels[mask_a].copy()\n",
    "        landms_t = landm[mask_a].copy()\n",
    "        landms_t = landms_t.reshape([-1, 5, 2])\n",
    "\n",
    "        if boxes_t.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        image_t = image[roi[1]:roi[3], roi[0]:roi[2]]\n",
    "\n",
    "        boxes_t[:, :2] = np.maximum(boxes_t[:, :2], roi[:2])\n",
    "        boxes_t[:, :2] -= roi[:2]\n",
    "        boxes_t[:, 2:] = np.minimum(boxes_t[:, 2:], roi[2:])\n",
    "        boxes_t[:, 2:] -= roi[:2]\n",
    "\n",
    "        # landm\n",
    "        landms_t[:, :, :2] = landms_t[:, :, :2] - roi[:2]\n",
    "        landms_t[:, :, :2] = np.maximum(landms_t[:, :, :2], np.array([0, 0]))\n",
    "        landms_t[:, :, :2] = np.minimum(landms_t[:, :, :2], roi[2:] - roi[:2])\n",
    "        landms_t = landms_t.reshape([-1, 10])\n",
    "\n",
    "\n",
    "\t# make sure that the cropped image contains at least one face > 16 pixel at training image scale\n",
    "        b_w_t = (boxes_t[:, 2] - boxes_t[:, 0] + 1) / w * img_dim\n",
    "        b_h_t = (boxes_t[:, 3] - boxes_t[:, 1] + 1) / h * img_dim\n",
    "        mask_b = np.minimum(b_w_t, b_h_t) > 0.0\n",
    "        boxes_t = boxes_t[mask_b]\n",
    "        labels_t = labels_t[mask_b]\n",
    "        landms_t = landms_t[mask_b]\n",
    "\n",
    "        if boxes_t.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        pad_image_flag = False\n",
    "\n",
    "        return image_t, boxes_t, labels_t, landms_t, pad_image_flag\n",
    "    return image, boxes, labels, landm, pad_image_flag\n",
    "\n",
    "\n",
    "def _distort(image):\n",
    "\n",
    "    def _convert(image, alpha=1, beta=0):\n",
    "        tmp = image.astype(float) * alpha + beta\n",
    "        tmp[tmp < 0] = 0\n",
    "        tmp[tmp > 255] = 255\n",
    "        image[:] = tmp\n",
    "\n",
    "    image = image.copy()\n",
    "\n",
    "    if random.randrange(2):\n",
    "\n",
    "        #brightness distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, beta=random.uniform(-32, 32))\n",
    "\n",
    "        #contrast distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #saturation distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        #hue distortion\n",
    "        if random.randrange(2):\n",
    "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
    "            tmp %= 180\n",
    "            image[:, :, 0] = tmp\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    else:\n",
    "\n",
    "        #brightness distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, beta=random.uniform(-32, 32))\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        #saturation distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image[:, :, 1], alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "        #hue distortion\n",
    "        if random.randrange(2):\n",
    "            tmp = image[:, :, 0].astype(int) + random.randint(-18, 18)\n",
    "            tmp %= 180\n",
    "            image[:, :, 0] = tmp\n",
    "\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "        #contrast distortion\n",
    "        if random.randrange(2):\n",
    "            _convert(image, alpha=random.uniform(0.5, 1.5))\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def _expand(image, boxes, fill, p):\n",
    "    if random.randrange(2):\n",
    "        return image, boxes\n",
    "\n",
    "    height, width, depth = image.shape\n",
    "\n",
    "    scale = random.uniform(1, p)\n",
    "    w = int(scale * width)\n",
    "    h = int(scale * height)\n",
    "\n",
    "    left = random.randint(0, w - width)\n",
    "    top = random.randint(0, h - height)\n",
    "\n",
    "    boxes_t = boxes.copy()\n",
    "    boxes_t[:, :2] += (left, top)\n",
    "    boxes_t[:, 2:] += (left, top)\n",
    "    expand_image = np.empty(\n",
    "        (h, w, depth),\n",
    "        dtype=image.dtype)\n",
    "    expand_image[:, :] = fill\n",
    "    expand_image[top:top + height, left:left + width] = image\n",
    "    image = expand_image\n",
    "\n",
    "    return image, boxes_t\n",
    "\n",
    "\n",
    "def _mirror(image, boxes, landms):\n",
    "    _, width, _ = image.shape\n",
    "    if random.randrange(2):\n",
    "        image = image[:, ::-1]\n",
    "        boxes = boxes.copy()\n",
    "        boxes[:, 0::2] = width - boxes[:, 2::-2]\n",
    "\n",
    "        # landm\n",
    "        landms = landms.copy()\n",
    "        landms = landms.reshape([-1, 5, 2])\n",
    "        landms[:, :, 0] = width - landms[:, :, 0]\n",
    "        tmp = landms[:, 1, :].copy()\n",
    "        landms[:, 1, :] = landms[:, 0, :]\n",
    "        landms[:, 0, :] = tmp\n",
    "        tmp1 = landms[:, 4, :].copy()\n",
    "        landms[:, 4, :] = landms[:, 3, :]\n",
    "        landms[:, 3, :] = tmp1\n",
    "        landms = landms.reshape([-1, 10])\n",
    "\n",
    "    return image, boxes, landms\n",
    "\n",
    "\n",
    "def _pad_to_square(image, rgb_mean, pad_image_flag):\n",
    "    if not pad_image_flag:\n",
    "        return image\n",
    "    height, width, _ = image.shape\n",
    "    long_side = max(width, height)\n",
    "    image_t = np.empty((long_side, long_side, 3), dtype=image.dtype)\n",
    "    image_t[:, :] = rgb_mean\n",
    "    image_t[0:0 + height, 0:0 + width] = image\n",
    "    return image_t\n",
    "\n",
    "\n",
    "def _resize_subtract_mean(image, insize, rgb_mean):\n",
    "    interp_methods = [cv2.INTER_LINEAR, cv2.INTER_CUBIC, cv2.INTER_AREA, cv2.INTER_NEAREST, cv2.INTER_LANCZOS4]\n",
    "    interp_method = interp_methods[random.randrange(5)]\n",
    "    image = cv2.resize(image, (insize, insize), interpolation=interp_method)\n",
    "    image = image.astype(np.float32)\n",
    "    image -= rgb_mean\n",
    "    return image.transpose(2, 0, 1)\n",
    "\n",
    "\n",
    "class preproc(object):\n",
    "\n",
    "    def __init__(self, img_dim, rgb_means):\n",
    "        self.img_dim = img_dim\n",
    "        self.rgb_means = rgb_means\n",
    "\n",
    "    def __call__(self, image, targets):\n",
    "        assert targets.shape[0] > 0, \"this image does not have gt\"\n",
    "\n",
    "        boxes = targets[:, :4].copy()\n",
    "        labels = targets[:, -1].copy()\n",
    "        landm = targets[:, 4:-1].copy()\n",
    "\n",
    "        image_t, boxes_t, labels_t, landm_t, pad_image_flag = _crop(image, boxes, labels, landm, self.img_dim)\n",
    "        image_t = _distort(image_t)\n",
    "        image_t = _pad_to_square(image_t,self.rgb_means, pad_image_flag)\n",
    "        image_t, boxes_t, landm_t = _mirror(image_t, boxes_t, landm_t)\n",
    "        height, width, _ = image_t.shape\n",
    "        image_t = _resize_subtract_mean(image_t, self.img_dim, self.rgb_means)\n",
    "        boxes_t[:, 0::2] /= width\n",
    "        boxes_t[:, 1::2] /= height\n",
    "\n",
    "        landm_t[:, 0::2] /= width\n",
    "        landm_t[:, 1::2] /= height\n",
    "\n",
    "        labels_t = np.expand_dims(labels_t, 1)\n",
    "        targets_t = np.hstack((boxes_t, landm_t, labels_t))\n",
    "\n",
    "        return image_t, targets_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# %load Pytorch_Retinaface/data/wider_face.py\n",
    "import os\n",
    "import os.path\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "class WiderFaceDetection(data.Dataset):\n",
    "    def __init__(self, txt_path, preproc=None):\n",
    "        self.preproc = preproc\n",
    "        self.imgs_path = []\n",
    "        self.words = []\n",
    "        f = open(txt_path,'r')\n",
    "        lines = f.readlines()\n",
    "        isFirst = True\n",
    "        labels = []\n",
    "        for line in lines:\n",
    "            line = line.rstrip()\n",
    "            if line.startswith('#'):\n",
    "                if isFirst is True:\n",
    "                    isFirst = False\n",
    "                else:\n",
    "                    labels_copy = labels.copy()\n",
    "                    self.words.append(labels_copy)\n",
    "                    labels.clear()\n",
    "                path = line[2:]\n",
    "                path = txt_path.replace('label.txt','images/') + path\n",
    "                self.imgs_path.append(path)\n",
    "            else:\n",
    "                line = line.split(' ')\n",
    "                label = [float(x) for x in line]\n",
    "                labels.append(label)\n",
    "\n",
    "        self.words.append(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = cv2.imread(self.imgs_path[index])\n",
    "        height, width, _ = img.shape\n",
    "\n",
    "        labels = self.words[index]\n",
    "        annotations = np.zeros((0, 15))\n",
    "        if len(labels) == 0:\n",
    "            return annotations\n",
    "        for idx, label in enumerate(labels):\n",
    "            annotation = np.zeros((1, 15))\n",
    "            # bbox\n",
    "            annotation[0, 0] = label[0]  # x1\n",
    "            annotation[0, 1] = label[1]  # y1\n",
    "            annotation[0, 2] = label[0] + label[2]  # x2\n",
    "            annotation[0, 3] = label[1] + label[3]  # y2\n",
    "\n",
    "            # landmarks\n",
    "            annotation[0, 4] = label[4]    # l0_x\n",
    "            annotation[0, 5] = label[5]    # l0_y\n",
    "            annotation[0, 6] = label[7]    # l1_x\n",
    "            annotation[0, 7] = label[8]    # l1_y\n",
    "            annotation[0, 8] = label[10]   # l2_x\n",
    "            annotation[0, 9] = label[11]   # l2_y\n",
    "            annotation[0, 10] = label[13]  # l3_x\n",
    "            annotation[0, 11] = label[14]  # l3_y\n",
    "            annotation[0, 12] = label[16]  # l4_x\n",
    "            annotation[0, 13] = label[17]  # l4_y\n",
    "            if (annotation[0, 4]<0):\n",
    "                annotation[0, 14] = -1\n",
    "            else:\n",
    "                annotation[0, 14] = 1\n",
    "\n",
    "            annotations = np.append(annotations, annotation, axis=0)\n",
    "        target = np.array(annotations)\n",
    "        if self.preproc is not None:\n",
    "            img, target = self.preproc(img, target)\n",
    "\n",
    "        return torch.from_numpy(img), target\n",
    "\n",
    "def detection_collate(batch):\n",
    "    \"\"\"Custom collate fn for dealing with batches of images that have a different\n",
    "    number of associated object annotations (bounding boxes).\n",
    "\n",
    "    Arguments:\n",
    "        batch: (tuple) A tuple of tensor images and lists of annotations\n",
    "\n",
    "    Return:\n",
    "        A tuple containing:\n",
    "            1) (tensor) batch of images stacked on their 0 dim\n",
    "            2) (list of tensors) annotations for a given image are stacked on 0 dim\n",
    "    \"\"\"\n",
    "    targets = []\n",
    "    imgs = []\n",
    "    for _, sample in enumerate(batch):\n",
    "        for _, tup in enumerate(sample):\n",
    "            if torch.is_tensor(tup):\n",
    "                imgs.append(tup)\n",
    "            elif isinstance(tup, type(np.empty(0))):\n",
    "                annos = torch.from_numpy(tup).float()\n",
    "                targets.append(annos)\n",
    "\n",
    "    return (torch.stack(imgs, 0), targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbdev.export import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_data.ipynb.\n",
      "Converted 00a_video_compression.ipynb.\n",
      "Converted 01_face_detection.ipynb.\n",
      "Converted 01a_faces_probs_examples.ipynb.\n",
      "Converted 01a_faces_probs_examples_hv.ipynb.\n",
      "Converted 02_fix_luminosity.ipynb.\n",
      "Converted 02a_create_faceimage_dataset.ipynb.\n",
      "Converted 02bis_Create_Dataset.ipynb.\n",
      "Converted 02c_faces_different_dfdc_zips.ipynb.\n",
      "Converted 03_models.ipynb.\n",
      "Converted 04_Baseline_Classification.ipynb.\n",
      "Converted 04_Classification.ipynb.\n",
      "Converted 04a_classification_videolist.ipynb.\n",
      "Converted 04b_inference.ipynb.\n",
      "Converted 05_Class_Imbalance.ipynb.\n",
      "Converted 06_Focal_Loss.ipynb.\n",
      "Converted 07_full_classification.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted 07a_classify_video_margin.ipynb.\n",
      "Converted 07b_classify_resize.ipynb.\n",
      "Converted 08_Validation.ipynb.\n",
      "Converted 09_DataAugmentation.ipynb.\n",
      "Converted 10_organising_face_detection_results.ipynb.\n",
      "Converted 11_fake_original.ipynb.\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "This cell doesn't have an export destination and was ignored:\n",
      "e\n",
      "Converted Create_Validation_Set.ipynb.\n",
      "Converted EasyRetinaFace.ipynb.\n",
      "Converted classifier_training.ipynb.\n",
      "Converted deepfake_submission.ipynb.\n",
      "Converted example_-_generating_origfake_dataset-1.ipynb.\n",
      "Converted example_-_generating_origfake_dataset.ipynb.\n",
      "Converted export_kernel_module.ipynb.\n",
      "Converted online_selection.ipynb.\n",
      "Converted online_selection_mnist_tiny.ipynb.\n",
      "Converted pytorch_retinaface.data.ipynb.\n",
      "Converted pytorch_retinaface.layers.functions.prior_box.ipynb.\n",
      "Converted pytorch_retinaface.models.net.ipynb.\n",
      "Converted pytorch_retinaface.models.retinaface.ipynb.\n",
      "Converted pytorch_retinaface.test_widerface.ipynb.\n",
      "Converted pytorch_retinaface.utils.box_utils.ipynb.\n",
      "Converted pytorch_retinaface.utils.nms.py_cpu_nms.ipynb.\n",
      "Converted pytorch_retinaface.utils.timer.ipynb.\n",
      "Converted test_submission.ipynb.\n"
     ]
    }
   ],
   "source": [
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
