{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export module for Kaggle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for gathering all the definitions (`def`s, `class`s, etc.) that will be needed in the submission kernel into a single `.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Path('../kgl_deepfake/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames_src = get_files(SRC, extensions='.py')\n",
    "fnames_src = [fn for fn in fnames_src if not fn.name.startswith('_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPORT_LINES = ('from', 'import')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps, defs = [], []\n",
    "for fname in sorted(fnames_src):\n",
    "    ls = open(fname).readlines()[4:]\n",
    "    ls = [l for l in ls if not l.startswith('# Cell')]\n",
    "    ls = [l+'\\n' if not l.endswith('\\n') else l for l in ls]\n",
    "    imps_ls = [l for l in ls if l.startswith(IMPORT_LINES)]\n",
    "    defs_ls = [l for l in ls if not l.startswith(IMPORT_LINES)]\n",
    "    imps.extend(imps_ls); defs.extend(defs_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "imps = sorted([l for l in set(imps) if not l.startswith('from .nb_')])\n",
    "imps = [l for l in imps if not l.startswith('from nbdev')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('kernel_module.py', mode='w', encoding='utf-8') as f:\n",
    "    f.write(''.join(imps + defs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from IPython.display import HTML\r\n",
      "from IPython.display import display, Video, HTML\r\n",
      "from facenet_pytorch import MTCNN\r\n",
      "from fastai.core import *\r\n",
      "from fastai.vision import *\r\n",
      "from tqdm import tqdm\r\n",
      "import cv2\r\n",
      "import pandas as pd\r\n",
      "import time\r\n",
      "import torch.nn as nn\r\n",
      "import torch.nn.functional as F\r\n",
      "\r\n",
      "\r\n",
      "def html_vid(fname, **kwargs): return display(Video(fname, **kwargs))\r\n",
      "\r\n",
      "def html_vid(fname):\r\n",
      "    \"Return HTML for video.\"\r\n",
      "    return f'''\r\n",
      "    <video width=\"300\" height=\"250\" controls>\r\n",
      "    <source src=\"{fname}\" type=\"video/mp4\">\r\n",
      "    </video>\r\n",
      "    '''\r\n",
      "\r\n",
      "def html_titled_vid(fname, title):\r\n",
      "    \"Return HTML for titled video.\"\r\n",
      "    return f'<div><p>{title}</p><br>{html_vid(fname)}</div>'\r\n",
      "\r\n",
      "def html_vids(fnames, titles=None, ncols=3):\r\n",
      "    \"Return HTML for table of (titled) videos.\"\r\n",
      "    n = len(fnames)\r\n",
      "    if titles is None: titles = n * ['']\r\n",
      "    assert len(titles) == n\r\n",
      "    rs = []\r\n",
      "    for i in range(0, n, ncols):\r\n",
      "        fs, ts = fnames[i:i+ncols], titles[i:i+ncols]\r\n",
      "        xs = (html_titled_vid(f, t) for f,t in zip(fs, ts))\r\n",
      "        xs = (f'<td>{x}</td>' for x in xs)\r\n",
      "        r = f\"<tr>{''.join(xs)}</tr>\"\r\n",
      "        rs.append(r)\r\n",
      "    return f\"<table>{''.join(rs)}</table>\"\r\n",
      "\r\n",
      "def video2frames(fname, *fs):\r\n",
      "    '''\r\n",
      "    fname - path to mp4 file\r\n",
      "    fs - fractional lengths to resize original image to. e.g. (.5, .25)\r\n",
      "    '''\r\n",
      "    capture = cv2.VideoCapture(fname)\r\n",
      "    imgs = []\r\n",
      "    for i in tqdm(range(int(capture.get(cv2.CAP_PROP_FRAME_COUNT)))):\r\n",
      "        _, img0 = capture.read()\r\n",
      "        img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB)\r\n",
      "        imgs.append([img0] + [cv2.resize(img0, (0, 0), fx=f, fy=f) for f in fs])\r\n",
      "    capture.release()\r\n",
      "    return [np.stack(imgsz) for imgsz in zip(*imgs)]\r\n",
      "\r\n",
      "def detect_facenet_pytorch(detector, images, batch_size):\r\n",
      "    '''\r\n",
      "    detector - facenet_pytorch.MTCNN\r\n",
      "    images:  numpy.array\r\n",
      "      array of images\r\n",
      "    batch_size: int\r\n",
      "      Number of images to be processed by `detector` in one go.\r\n",
      "    '''\r\n",
      "    faces = []\r\n",
      "    for lb in np.arange(0, len(images), batch_size):\r\n",
      "        imgs_pil = [PIL.Image.fromarray(image) for image in images[lb:lb+batch_size]]\r\n",
      "        faces.extend(detector(imgs_pil))\r\n",
      "    return torch.stack(faces)\r\n",
      "\r\n",
      "\r\n",
      "#comes from https://www.kaggle.com/unkownhihi/starter-kernel-with-cnn-model-ll-lb-0-69235\r\n",
      "class DetectionPipeline:\r\n",
      "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\r\n",
      "\r\n",
      "    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\r\n",
      "        \"\"\"Constructor for DetectionPipeline class.\r\n",
      "\r\n",
      "        Keyword Arguments:\r\n",
      "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\r\n",
      "                throughout the video. If not specified (i.e., None), all frames will be loaded.\r\n",
      "                (default: {None})\r\n",
      "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\r\n",
      "            resize {float} -- Fraction by which to resize frames from original prior to face\r\n",
      "                detection. A value less than 1 results in downsampling and a value greater than\r\n",
      "                1 result in upsampling. (default: {None})\r\n",
      "        \"\"\"\r\n",
      "        self.detector = detector\r\n",
      "        self.n_frames, self.batch_size, self.resize = n_frames, batch_size, resize\r\n",
      "\r\n",
      "    def __call__(self, filename, label=None, save_dir=None):\r\n",
      "        \"\"\"Load frames from an MP4 video and detect faces.\r\n",
      "        Arguments:\r\n",
      "            filename {str} -- Path to video.\r\n",
      "        \"\"\"\r\n",
      "        v_cap = cv2.VideoCapture(filename)\r\n",
      "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n",
      "\r\n",
      "        if self.n_frames is None: sample = np.arange(0, v_len)\r\n",
      "        else: sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\r\n",
      "\r\n",
      "        faces = []\r\n",
      "        idxs, frames = [], []\r\n",
      "        for j in range(v_len):\r\n",
      "            success = v_cap.grab()\r\n",
      "            if j in sample:\r\n",
      "                success, frame = v_cap.retrieve()\r\n",
      "                if not success: continue\r\n",
      "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n",
      "                frame = PIL.Image.fromarray(frame)\r\n",
      "\r\n",
      "                if self.resize is not None:\r\n",
      "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\r\n",
      "                idxs.append(j); frames.append(frame)\r\n",
      "\r\n",
      "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\r\n",
      "                    if save_dir is not None:\r\n",
      "                        save_paths = self.get_savepaths(filename, idxs, label, save_dir)\r\n",
      "                        faces.extend(self.detector(frames, save_path=save_paths))\r\n",
      "                    else: faces.extend(self.detector(frames))\r\n",
      "                    idxs, frames = [], []\r\n",
      "\r\n",
      "        v_cap.release()\r\n",
      "        return faces\r\n",
      "\r\n",
      "    def get_savepaths(self, filename, idxs, label=None, save_dir=None):\r\n",
      "        if isinstance(filename, str): filename = Path(filename)\r\n",
      "        if save_dir is None: save_dir = Path('./')\r\n",
      "        if label is None: save_paths = [save_dir/f'{filename.stem}_{i:03d}.png' for i in idxs]\r\n",
      "        else: save_paths = [save_dir/f'{filename.stem}_{i:03d}_{label}.png' for i in idxs]\r\n",
      "        return [str(o) for o in save_paths]\r\n",
      "\r\n",
      "def get_first_face(detector, fn, resize=.5):\r\n",
      "    '''\r\n",
      "    Returns the first detected face from a video\r\n",
      "    '''\r\n",
      "    v_cap = cv2.VideoCapture(str(fn))\r\n",
      "    v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n",
      "    iframe, face = None, None\r\n",
      "    for i in range(v_len):\r\n",
      "        _ = v_cap.grab()\r\n",
      "        success, frame = v_cap.retrieve()\r\n",
      "        if not success: continue\r\n",
      "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n",
      "        frame = PIL.Image.fromarray(frame)\r\n",
      "        if resize is not None: frame = frame.resize([int(d * resize) for d in frame.size])\r\n",
      "        face = detector(frame)\r\n",
      "        if face is not None:\r\n",
      "            iframe = i\r\n",
      "            break\r\n",
      "        v_cap.release()\r\n",
      "    return iframe, face\r\n",
      "\r\n",
      "def get_has_face(fnames, detector):\r\n",
      "    if isinstance(fnames, (str, Path)): fnames = [fnames]\r\n",
      "    res = []\r\n",
      "    for i in progress_bar(range(len(fnames))):\r\n",
      "        iframe, face = get_first_face(detector, fnames[i])\r\n",
      "        res.append(True if iframe is not None else False)\r\n",
      "    return res\r\n",
      "\r\n",
      "class VideoFaceList(ImageList):\r\n",
      "    def __init__(self, *args, detector=None, resize=.5, device=None, **kwargs):\r\n",
      "        if device is None: device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\r\n",
      "        if detector is None: detector = MTCNN(device=device, post_process=False)\r\n",
      "        self.detector, self.resize = detector, resize\r\n",
      "        super().__init__(*args, **kwargs)\r\n",
      "\r\n",
      "    def get_face(self, fn:Path):\r\n",
      "        iframe, face = get_first_face(self.detector, fn, self.resize)\r\n",
      "        if iframe is None or face is None: raise Exception(f'No faces detected in {fn}')\r\n",
      "        return iframe, face\r\n",
      "\r\n",
      "    def open(self, fn:Path):\r\n",
      "        iframe, face = self.get_face(fn)\r\n",
      "        return Image(face / 255)\r\n",
      "\r\n",
      "def show_faces(faces, idxs):\r\n",
      "    ncols = 4\r\n",
      "    nrows = int((len(idxs) - 1) / ncols) + 1\r\n",
      "    _, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(18, 4*nrows))\r\n",
      "    for idx, ax in itertools.zip_longest(idxs, axs.flatten()):\r\n",
      "        if idx:\r\n",
      "            #img = np.array(faces[idx])\r\n",
      "            ax.imshow(diff[idx].permute(1, 2, 0)/255.); ax.set_title(idx)\r\n",
      "        ax.axis('off')\r\n",
      "\r\n",
      "def plot_faces(faces, idxs):\r\n",
      "    ncols = 4\r\n",
      "    nrows = int((len(idxs) - 1) / ncols) + 1\r\n",
      "    _, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=(18, 4*nrows))\r\n",
      "    for idx, ax in itertools.zip_longest(idxs, axs.flatten()):\r\n",
      "        if idx:\r\n",
      "            #img = np.array(faces[idx])\r\n",
      "            ax.imshow(diff[idx].permute(1, 2, 0)/255.); ax.set_title(idx)\r\n",
      "        ax.axis('off')\r\n",
      "\r\n",
      "\r\n",
      "#comes from https://www.kaggle.com/unkownhihi/starter-kernel-with-cnn-model-ll-lb-0-69235\r\n",
      "class DetectionPipeline:\r\n",
      "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\r\n",
      "\r\n",
      "    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\r\n",
      "        \"\"\"Constructor for DetectionPipeline class.\r\n",
      "\r\n",
      "        Keyword Arguments:\r\n",
      "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\r\n",
      "                throughout the video. If not specified (i.e., None), all frames will be loaded.\r\n",
      "                (default: {None})\r\n",
      "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\r\n",
      "            resize {float} -- Fraction by which to resize frames from original prior to face\r\n",
      "                detection. A value less than 1 results in downsampling and a value greater than\r\n",
      "                1 result in upsampling. (default: {None})\r\n",
      "        \"\"\"\r\n",
      "        self.detector = detector\r\n",
      "        self.n_frames = n_frames\r\n",
      "        self.batch_size = batch_size\r\n",
      "        self.resize = resize\r\n",
      "\r\n",
      "    def __call__(self, filename):\r\n",
      "        \"\"\"Load frames from an MP4 video and detect faces.\r\n",
      "\r\n",
      "        Arguments:\r\n",
      "            filename {str} -- Path to video.\r\n",
      "        \"\"\"\r\n",
      "        # Create video reader and find length\r\n",
      "        v_cap = cv2.VideoCapture(filename)\r\n",
      "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\r\n",
      "\r\n",
      "        # Pick 'n_frames' evenly spaced frames to sample\r\n",
      "        if self.n_frames is None:\r\n",
      "            sample = np.arange(0, v_len)\r\n",
      "        else:\r\n",
      "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\r\n",
      "\r\n",
      "        # Loop through frames\r\n",
      "        faces = []\r\n",
      "        frames = []\r\n",
      "        for j in range(v_len):\r\n",
      "            success = v_cap.grab()\r\n",
      "            if j in sample:\r\n",
      "                # Load frame\r\n",
      "                success, frame = v_cap.retrieve()\r\n",
      "                if not success:\r\n",
      "                    continue\r\n",
      "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\r\n",
      "                frame = Image.fromarray(frame)\r\n",
      "\r\n",
      "                # Resize frame to desired size\r\n",
      "                if self.resize is not None:\r\n",
      "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\r\n",
      "                frames.append(frame)\r\n",
      "\r\n",
      "                # When batch is full, detect faces and reset frame list\r\n",
      "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\r\n",
      "                    faces.extend(self.detector(frames))\r\n",
      "                    frames = []\r\n",
      "\r\n",
      "        v_cap.release()\r\n",
      "\r\n",
      "        return faces\r\n",
      "\r\n",
      "# By Nathan Hubens.\r\n",
      "# Paper implementation does not use Adaptive Average Pooling. To get the exact same implementation,\r\n",
      "# comment the avg_pool and uncomment the final max_pool layer.\r\n",
      "class MesoNet(nn.Module):\r\n",
      "    def __init__(self):\r\n",
      "        super().__init__()\r\n",
      "\r\n",
      "        self.conv1 = nn.Conv2d(3, 8, 3, 1,1) # 8 x 256 x 256\r\n",
      "        self.bn1 = nn.BatchNorm2d(8)\r\n",
      "        self.conv2 = nn.Conv2d(8, 8, 5, 1,2) # 8 x 128 x 128\r\n",
      "        self.bn2 = nn.BatchNorm2d(8)\r\n",
      "        self.conv3 = nn.Conv2d(8, 16, 5, 1,2) # 8 x 64 x 64\r\n",
      "        self.bn3 = nn.BatchNorm2d(16)\r\n",
      "        self.conv4 = nn.Conv2d(16,16,5,1,2) # 8 x 32 x 32\r\n",
      "        self.bn4 = nn.BatchNorm2d(16)\r\n",
      "        self.avg_pool = nn.AdaptiveAvgPool2d((8))\r\n",
      "        self.fc1 = nn.Linear(1024, 16)\r\n",
      "        self.fc2 = nn.Linear(16, 2)\r\n",
      "\r\n",
      "    def forward(self, x):\r\n",
      "\r\n",
      "        x = F.relu(self.conv1(x))\r\n",
      "        x = self.bn1(x)\r\n",
      "        x = F.max_pool2d(x, 2, 2)\r\n",
      "\r\n",
      "        x = F.relu(self.conv2(x))\r\n",
      "        x = self.bn2(x)\r\n",
      "        x = F.max_pool2d(x, 2, 2)\r\n",
      "\r\n",
      "        x = F.relu(self.conv3(x))\r\n",
      "        x = self.bn3(x)\r\n",
      "        x = F.max_pool2d(x, 2, 2)\r\n",
      "\r\n",
      "        x = F.relu(self.conv4(x))\r\n",
      "        x = self.bn4(x)\r\n",
      "        #x = F.max_pool2d(x, 4, 4)\r\n",
      "\r\n",
      "        x = self.avg_pool(x)\r\n",
      "\r\n",
      "        x = x.reshape(x.shape[0], -1)\r\n",
      "\r\n",
      "        x = F.dropout(x, 0.5)\r\n",
      "        x = F.relu(self.fc1(x))\r\n",
      "        x = F.dropout(x,0.5)\r\n",
      "        x = self.fc2(x)\r\n",
      "        return x\r\n",
      "\r\n",
      "def infer_on_videolist(learn:Learner, vlist:VideoFaceList):\r\n",
      "    filenames, labels = [], []\r\n",
      "    for i in progress_bar(range(len(vlist))):\r\n",
      "        fn, img = vlist.items[i], vlist[i]\r\n",
      "        y, _, _ = learn.predict(img)\r\n",
      "        filenames.append(fn.name)\r\n",
      "        labels.append(int(y))\r\n",
      "    return pd.DataFrame({'filename':filenames, 'label':labels})\r\n",
      "\r\n",
      "class FocalLoss(nn.Module):\r\n",
      "    def __init__(self, alpha=1, gamma=2, logits=False, reduction='elementwise_mean'):\r\n",
      "        super(FocalLoss, self).__init__()\r\n",
      "        self.alpha = alpha\r\n",
      "        self.gamma = gamma\r\n",
      "        self.logits = logits\r\n",
      "        self.reduction = reduction\r\n",
      "\r\n",
      "    def forward(self, inputs, targets):\r\n",
      "        if self.logits:\r\n",
      "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\r\n",
      "        else:\r\n",
      "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\r\n",
      "        pt = torch.exp(-BCE_loss)\r\n",
      "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\r\n",
      "\r\n",
      "        if self.reduction is None:\r\n",
      "            return F_loss\r\n",
      "        else:\r\n",
      "            return torch.mean(F_loss)\r\n"
     ]
    }
   ],
   "source": [
    "! cat kernel_module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
