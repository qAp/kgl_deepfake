{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Raw Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have generated a number of predictions and now we would like to investigate different ways of distilling multiple frame predictions into a single video prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from pathlib import Path\n",
    "from video_utils import load_all_metadata\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6266256e-05, 1.3985855e-05, 4.4961173e-05, 1.6710995e-04,\n",
       "       1.5438546e-04, 1.5894193e-03, 1.8297692e-04, 3.0514985e-04,\n",
       "       1.9729466e-04, 6.4241159e-01, 5.3111835e-05, 6.9973656e-05,\n",
       "       7.9855403e-05, 1.3117793e-04, 2.1690319e-04, 1.2356082e-04,\n",
       "       2.7763998e-04], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_preds = np.load('raw_preds.npy', allow_pickle=True).item()\n",
    "\n",
    "# Example\n",
    "first_key = list(raw_preds.keys())[0]\n",
    "raw_preds[first_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f3c7e6ce5446968d4d7a756a2b4cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_metadata= load_all_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This competition's [evaluation metric](https://www.kaggle.com/c/deepfake-detection-challenge/overview/evaluation) is log loss which is given as:\n",
    "\n",
    "$\\textrm{LogLoss} = - \\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)\\right]$\n",
    "\n",
    "We'll use sklearn's `log_loss()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1535237802567928"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate sample values\n",
    "y_true = []\n",
    "y_hat = []\n",
    "for i in range(10):\n",
    "    y_true.append(np.random.randint(2)) # int of 0 or 1\n",
    "    y_hat.append(np.random.rand())      # float between 0 and 1\n",
    "\n",
    "# Get loss with sklearn\n",
    "log_loss(y_true, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious and straightforward approach might be to simply average all of our raw predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24043698457560553"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = []\n",
    "y_true = []\n",
    "\n",
    "for path, preds in raw_preds.items():\n",
    "    # Note that we clip values\n",
    "    avg = np.mean(preds).clip(0.01, 0.99)\n",
    "    avg_preds.append(avg)\n",
    "    \n",
    "    y = 1 if all_metadata.loc[all_metadata['fname'] == path]['label'].iloc[0] == 'FAKE' else 0\n",
    "    y_true.append(y)\n",
    "    \n",
    "log_loss(y_true, avg_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Max Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes a single frame is enough evidence that we've encountered a deep fake. Let's try taking the max value and what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6439748840178592"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_preds = []\n",
    "y_true = []\n",
    "\n",
    "for path, preds in raw_preds.items():\n",
    "    # Note that we clip values\n",
    "    avg = np.max(preds).clip(0.01, 0.99)\n",
    "    avg_preds.append(avg)\n",
    "    \n",
    "    y = 1 if all_metadata.loc[all_metadata['fname'] == path]['label'].iloc[0] == 'FAKE' else 0\n",
    "    y_true.append(y)\n",
    "    \n",
    "log_loss(y_true, avg_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much worse!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
