{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with Single Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have what looks to be a reasonably capable model and we would like to test it against the real test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josh/anaconda3/envs/dl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/josh/anaconda3/envs/dl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/josh/anaconda3/envs/dl/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from fastai.data_block import get_files\n",
    "from fastai.vision import Learner, load_learner, ImageList\n",
    "from fastai.metrics import accuracy\n",
    "\n",
    "from EasyBlazeFace import EasyBlazeFace\n",
    "from EasyRetinaFace import EasyRetinaFace\n",
    "from video_utils import plot_detections, read_frames, bb_intersection_over_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "__all__ = [\n",
    "    'ResNet', 'resnet10', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n",
    "    'resnet152', 'resnet200'\n",
    "]\n",
    "\n",
    "\n",
    "def conv3x3x3(in_planes, out_planes, stride=1):\n",
    "    # 3x3x3 convolution with padding\n",
    "    return nn.Conv3d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=1,\n",
    "        bias=False)\n",
    "\n",
    "\n",
    "def downsample_basic_block(x, planes, stride):\n",
    "    out = F.avg_pool3d(x, kernel_size=1, stride=stride)\n",
    "    zero_pads = torch.Tensor(\n",
    "        out.size(0), planes - out.size(1), out.size(2), out.size(3),\n",
    "        out.size(4)).zero_()\n",
    "    if isinstance(out.data, torch.cuda.FloatTensor):\n",
    "        zero_pads = zero_pads.cuda()\n",
    "\n",
    "    out = Variable(torch.cat([out.data, zero_pads], dim=1))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(planes)\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(planes)\n",
    "        self.conv3 = nn.Conv3d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm3d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 block,\n",
    "                 layers,\n",
    "                 sample_size,\n",
    "                 sample_duration,\n",
    "                 shortcut_type='B',\n",
    "                 num_classes=400):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        self.sample_duration = sample_duration\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            3,\n",
    "            64,\n",
    "            kernel_size=7,\n",
    "            stride=(1, 2, 2),\n",
    "            padding=(3, 3, 3),\n",
    "            bias=False)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0], shortcut_type)\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, 128, layers[1], shortcut_type, stride=2)\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, 256, layers[2], shortcut_type, stride=2)\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, 512, layers[3], shortcut_type, stride=2)\n",
    "        \n",
    "        last_duration = int(math.ceil(sample_duration / 16))\n",
    "        last_size = int(math.ceil(sample_size / 32))\n",
    "        self.avgpool = nn.AvgPool3d(\n",
    "            (last_duration, last_size, last_size), stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                m.weight = nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, shortcut_type, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            if shortcut_type == 'A':\n",
    "                downsample = partial(\n",
    "                    downsample_basic_block,\n",
    "                    planes=planes * block.expansion,\n",
    "                    stride=stride)\n",
    "            else:\n",
    "                downsample = nn.Sequential(\n",
    "                    nn.Conv3d(\n",
    "                        self.inplanes,\n",
    "                        planes * block.expansion,\n",
    "                        kernel_size=1,\n",
    "                        stride=stride,\n",
    "                        bias=False), nn.BatchNorm3d(planes * block.expansion))\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if len(x.shape) == 4:\n",
    "            batch_size, stacked_size, height, width = x.shape\n",
    "            rgb_channels = 3\n",
    "\n",
    "            # fastai requires that inputs be of shape:\n",
    "            # (BATCH, CHANNELS * NUM_FRAMES, HEIGHT, WIDTH)\n",
    "            # PyTorch's 3D convolution operations require that inputs be of shape:\n",
    "            # (BATCH, CHANNELS, NUM_FRAMES, HEIGHT, WIDTH)\n",
    "            x = x.view(batch_size, 16, rgb_channels, height, width).permute(0, 2,1,3,4)\n",
    "\n",
    "    #         print(x.shape) #torch.Size([64, 3, 10, 128, 128])\n",
    "    #         plt.imshow(x[0,:,0,:,:].permute(1,2,0))\n",
    "    #         plt.show()\n",
    "\n",
    "            # Reshaping and permuting puts x back on the CPU, so we'll move it back to the GPU.\n",
    "            x = x.cuda()\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_fine_tuning_parameters(model, ft_begin_index):\n",
    "    if ft_begin_index == 0:\n",
    "        return model.parameters()\n",
    "\n",
    "    ft_module_names = []\n",
    "    for i in range(ft_begin_index, 5):\n",
    "        ft_module_names.append('layer{}'.format(i))\n",
    "    ft_module_names.append('fc')\n",
    "\n",
    "    parameters = []\n",
    "    for k, v in model.named_parameters():\n",
    "        for ft_module in ft_module_names:\n",
    "            if ft_module in k:\n",
    "                parameters.append({'params': v})\n",
    "                break\n",
    "        else:\n",
    "            parameters.append({'paramsbatch_size, height, width, stacked_size = input.shape': v, 'lr': 0.0})\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def resnet10(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [1, 1, 1, 1], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet34(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-34 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet50(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet101(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 4, 23, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet152(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 8, 36, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet200(**kwargs):\n",
    "    \"\"\"Constructs a ResNet-101 model.\n",
    "    \"\"\"\n",
    "    model = ResNet(Bottleneck, [3, 24, 36, 3], **kwargs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model and weights\n",
    "sz = 256\n",
    "sample_duration = 16\n",
    "model = resnet18(sample_size=sz, sample_duration=sample_duration, num_classes=400, shortcut_type='A')\n",
    "# Adjust the last layer to our problems classification task\n",
    "model.fc = torch.nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "\n",
    "state_dict = torch.load('../data/16_frames/models/best.pth')\n",
    "model.load_state_dict(state_dict['model'])\n",
    "\n",
    "model = model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOURCE_TEST = Path('../input/deepfake-detection-challenge/test_videos/')\n",
    "# submission = pd.read_csv('../input/deepfake-detection-challenge/sample_submission.csv')\n",
    "SOURCE_TEST = Path('../data/cropped_faces/valid_videos/')\n",
    "submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_bounding_boxes_by_overlap(detections_for_frames, iou_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Many videos have multiple people in them which leads to multiple detections in a given frame.\n",
    "    This method takes a set of detections across multiple frames and attempts to group them\n",
    "    according to the person each detection represents.\n",
    "    \n",
    "    We break apart detections by comparing each detection to the IOU value of the detections in \n",
    "    previous frames. If there is enough intersection (ie. high IOU) we match that detection to \n",
    "    the one from the previous frame.\n",
    "    \"\"\"\n",
    "    \n",
    "    grouped_frame_detections = []\n",
    "    \n",
    "    for frame_detections in detections_for_frames:\n",
    "\n",
    "        for detections in frame_detections:\n",
    "\n",
    "            # Figure out where in `grouped_frame_detections` this belongs\n",
    "            best_iou = -1\n",
    "            best_index = -1\n",
    "\n",
    "            for i, group in enumerate(grouped_frame_detections):\n",
    "\n",
    "                # Get the most recent detection\n",
    "                last_detections = group[-1]\n",
    "\n",
    "                # Calculate iou\n",
    "                iou = bb_intersection_over_union(detections, last_detections)\n",
    "\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_index = i\n",
    "\n",
    "            if best_iou < iou_threshold:\n",
    "                # No suitable group was found, add a new one\n",
    "                grouped_frame_detections.append([detections])\n",
    "            else:\n",
    "                # Place in group\n",
    "                grouped_frame_detections[best_index].append(detections)\n",
    "                \n",
    "    return grouped_frame_detections    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_largest_bounding_box(detections_for_frames):\n",
    "    \"\"\"\n",
    "    Given a set of detections that span across multiple frames (but represent only ONE person)\n",
    "    return the largest bounding box that contains all other bounding boxes within it.\n",
    "    \"\"\"\n",
    "    \n",
    "    # A list of the detections for each face in the video.\n",
    "    # Each face has one set of coordinates that contains ALL of the bounding boxes from every frame.\n",
    "    largest_detections = []\n",
    "\n",
    "    # Get detections for the first frame\n",
    "    firstFrameDetections = detections_for_frames[0]\n",
    "    \n",
    "    x_min, y_min, x_max, y_max, _  = firstFrameDetections\n",
    "    largest_detections = [x_min, y_min, x_max, y_max]\n",
    "        \n",
    "    for detections in detections_for_frames[1:]:\n",
    "            \n",
    "        x_min, y_min, x_max, y_max, _ = detections\n",
    "\n",
    "        current_largest_detection = largest_detections\n",
    "        current_x_min, current_y_min, current_x_max, current_y_max = current_largest_detection\n",
    "\n",
    "        # Expand the bounding box if neccessary to include this one\n",
    "        current_x_min = min(x_min, current_x_min)\n",
    "        current_y_min = min(y_min, current_y_min)\n",
    "        current_x_max = max(x_max, current_x_max)\n",
    "        current_y_max = max(y_max, current_y_max)\n",
    "        \n",
    "        largest_detections = [current_x_min, current_y_min, current_x_max, current_y_max]\n",
    "\n",
    "    return largest_detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces_from_multiple_frames(frames, detections_for_frames, target_size=256):\n",
    "    \"\"\"\n",
    "    Given sequential random frames return any faces found within the frames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ignore empty detections\n",
    "    detections_for_frames = [x for x in detections_for_frames if len(x) != 0]\n",
    "\n",
    "    if len(detections_for_frames) == 0:\n",
    "        return []\n",
    "    \n",
    "    frame_height, frame_width, _ = frames[0].shape\n",
    "    \n",
    "    # Group detections by each person in the video\n",
    "    grouped_detections = group_bounding_boxes_by_overlap(detections_for_frames)\n",
    "    \n",
    "    all_faces = []\n",
    "    \n",
    "    for detected_group in grouped_detections:\n",
    "        largest_detections = get_largest_bounding_box(detected_group)\n",
    "\n",
    "        # Now that we have a set of detections, apply them against the frames and \n",
    "        # return only the portions of the frames that contain the face\n",
    "        x_min, y_min, x_max, y_max = largest_detections            \n",
    "        \n",
    "        # Make sure dets are within the frame when cropping\n",
    "        x_min = max(x_min, 0)\n",
    "        y_min = max(y_min, 0)\n",
    "        x_max = min(x_max, frame_width)\n",
    "        y_max = min(y_max, frame_height)\n",
    "        \n",
    "        face_frames = frames[:, int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "        \n",
    "        # Pre-allocate space for output faces\n",
    "        output_faces = np.zeros((len(face_frames), target_size, target_size, 3), dtype=np.uint8)\n",
    "\n",
    "        for i, face in enumerate(face_frames):\n",
    "    \n",
    "            # Resize to 256, 256\n",
    "            longest_size = np.max(face.shape[:2])\n",
    "            resize = float(target_size) / float(longest_size)\n",
    "            face = cv2.resize(face, None, None, fx=resize, fy=resize, interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Place within frame_faces (by default 0 where there is no image data)\n",
    "            height, width, _ = face.shape            \n",
    "            \n",
    "            output_faces[i, :height, :width, :] = face\n",
    "        \n",
    "        all_faces.append(output_faces)\n",
    "        \n",
    "    return all_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model from Pytorch_Retinaface/weights/Resnet50_Final.pth\n",
      "remove prefix 'module.'\n",
      "Missing keys:0\n",
      "Unused checkpoint keys:0\n",
      "Used keys:456\n"
     ]
    }
   ],
   "source": [
    "imagenet_mean = torch.Tensor([0.485, 0.456, 0.406]).to(device)\n",
    "imagenet_std = torch.Tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "#easyBlazeFace = EasyBlazeFace(weights='../input/blazeface/blazeface.pth', anchors='../input/blazeface/anchors.npy')\n",
    "easyBlazeFace = EasyBlazeFace()\n",
    "easyRetinaFace = EasyRetinaFace()\n",
    "\n",
    "def get_predictions_for_video(path):\n",
    "\n",
    "    frames = read_frames(path)\n",
    "    detections = easyBlazeFace.get_detections_with_multiple_crops(frames)\n",
    "    \n",
    "    grouped_faces = get_faces_from_multiple_frames(frames, detections)\n",
    "    \n",
    "    # If we cannot find any faces, try using EasyRetinaFace\n",
    "    if len(grouped_faces) == 0:\n",
    "        print(\"Trying RetinaFace for: \", path)\n",
    "        detections = easyRetinaFace.detect_on_multiple_frames(frames)\n",
    "\n",
    "        grouped_faces = get_faces_from_multiple_frames(frames, detections)\n",
    "        \n",
    "        if len(grouped_faces) == 0:\n",
    "            print(\"NO FACES FOR \", path)\n",
    "            return 0.5\n",
    "        \n",
    "    all_results = []\n",
    "    \n",
    "    for faces in grouped_faces:\n",
    "        \n",
    "        batched_faces = np.stack(faces, axis=0)\n",
    "\n",
    "        # Create tensor and divide by 255\n",
    "        input = (torch.from_numpy(batched_faces).float()) / 255.\n",
    "        \n",
    "        # N F H W C -> N C F H W\n",
    "        input = input.unsqueeze(0).permute(0, 4, 1, 2, 3).to(device)\n",
    "\n",
    "        # Normalize with ImageNet stats\n",
    "        # TODO: are we doing this in .forward()?\n",
    "        # input.sub_(imagenet_mean[None, :, None, None]).div_(imagenet_std[None, :, None, None])\n",
    "\n",
    "        results = torch.softmax(model(input), axis=-1).detach().cpu().numpy()\n",
    "        \n",
    "        all_results.append(results[:,0])\n",
    "\n",
    "    # TODO: Is there are a better way than just averaging across every person in the video?\n",
    "    return np.mean(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca0e4f806464d9883d91f1fb451f876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1148.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/xhegjwkfaa.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/ehkjdctavq.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/rlqbowounu.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/lkqotnclpd.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/xkicasophk.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/nycmyuzpml.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/iorbtaarte.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/gxbkcxyfjm.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/czifmcopho.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/olxuxttfce.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/wvbzxaspaa.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/kwfdyqofzw.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/nraqgmsnmm.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/hvyevrpeyc.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/caaackoecr.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/gxwmwjmyxr.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/bfeewgzrbr.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/fpevfidstw.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/lbigytrrtr.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/kbhqxcdcuf.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/ywxpquomgt.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/ncmpqwmnzb.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/zewluzlkbe.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/jiswxuqzyz.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/jkonwkrhqp.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/zzsvsjkcva.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/cbtcwfzudb.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/ugrlhuxwkw.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/olqctxmttw.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/xvpyayjsmf.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/imdmhwkkni.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/elackxuccp.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/slayysqett.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/krbdlemuzz.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/rukyxomwcx.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/pfoiiepvxb.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/ogghhskfdr.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/hqldzgqqpg.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/peysyddtmp.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/xwotyajtkq.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/ljauauuyka.mp4\n",
      "Trying RetinaFace for:  ../data/cropped_faces/valid_videos/pilxtozlrg.mp4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = {}\n",
    "\n",
    "for path in tqdm(SOURCE_TEST.ls()):\n",
    "    \n",
    "        results = get_predictions_for_video(path)\n",
    "        preds[path.name] = results\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save them for analysis\n",
    "np.save('retinaface_raw_preds.npy', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from video_utils import load_all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_preds = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24fefb3e1d964ef39fc65586232712a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "all_metadata = load_all_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>original</th>\n",
       "      <th>directory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>owxbbpjpch.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wynotylpnm.mp4</td>\n",
       "      <td>../data/dfdc_train_part_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/dfdc_train_part_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fzvpbrzssi.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/dfdc_train_part_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>htorvhbcae.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>wclvkepakb.mp4</td>\n",
       "      <td>../data/dfdc_train_part_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fckxaqjbxk.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>vpmyeepbep.mp4</td>\n",
       "      <td>../data/dfdc_train_part_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>pdooqxqfrm.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>ikebomnsiq.mp4</td>\n",
       "      <td>../data/dfdc_train_part_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>djjdcnhlma.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>kudvvlgiff.mp4</td>\n",
       "      <td>../data/dfdc_train_part_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>fgmbxfqoze.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/dfdc_train_part_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3132</th>\n",
       "      <td>cywebjaezn.mp4</td>\n",
       "      <td>REAL</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>../data/dfdc_train_part_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3133</th>\n",
       "      <td>ohmkmcfxul.mp4</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>train</td>\n",
       "      <td>hysmzkqsdl.mp4</td>\n",
       "      <td>../data/dfdc_train_part_49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>119154 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               fname label  split        original                   directory\n",
       "0     owxbbpjpch.mp4  FAKE  train  wynotylpnm.mp4   ../data/dfdc_train_part_0\n",
       "1     vpmyeepbep.mp4  REAL  train             NaN   ../data/dfdc_train_part_0\n",
       "2     fzvpbrzssi.mp4  REAL  train             NaN   ../data/dfdc_train_part_0\n",
       "3     htorvhbcae.mp4  FAKE  train  wclvkepakb.mp4   ../data/dfdc_train_part_0\n",
       "4     fckxaqjbxk.mp4  FAKE  train  vpmyeepbep.mp4   ../data/dfdc_train_part_0\n",
       "...              ...   ...    ...             ...                         ...\n",
       "3129  pdooqxqfrm.mp4  FAKE  train  ikebomnsiq.mp4  ../data/dfdc_train_part_49\n",
       "3130  djjdcnhlma.mp4  FAKE  train  kudvvlgiff.mp4  ../data/dfdc_train_part_49\n",
       "3131  fgmbxfqoze.mp4  REAL  train             NaN  ../data/dfdc_train_part_49\n",
       "3132  cywebjaezn.mp4  REAL  train             NaN  ../data/dfdc_train_part_49\n",
       "3133  ohmkmcfxul.mp4  FAKE  train  hysmzkqsdl.mp4  ../data/dfdc_train_part_49\n",
       "\n",
       "[119154 rows x 5 columns]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folder 0 0.39684384101631764\n",
      "folder 1 0.3635189005380322\n",
      "folder 2 0.38617420159061233\n",
      "all 0.3823777159509993\n"
     ]
    }
   ],
   "source": [
    "folder_0_avg_preds = []\n",
    "folder_0_y_true = []\n",
    "\n",
    "folder_1_avg_preds = []\n",
    "folder_1_y_true = []\n",
    "\n",
    "folder_2_avg_preds = []\n",
    "folder_2_y_true = []\n",
    "\n",
    "for path, preds in raw_preds.items():\n",
    "    # Note that we clip values\n",
    "    avg = np.mean(preds).clip(0.01, 0.99)\n",
    "\n",
    "    row = all_metadata.loc[all_metadata['fname'] == path].iloc[0]\n",
    "    \n",
    "    if row['directory'] == '../data/dfdc_train_part_0':\n",
    "        y_true = folder_0_y_true\n",
    "        avg_preds = folder_0_avg_preds\n",
    "    elif row['directory'] == '../data/dfdc_train_part_1':\n",
    "        y_true = folder_1_y_true\n",
    "        avg_preds = folder_1_avg_preds\n",
    "    elif row['directory'] == '../data/dfdc_train_part_2':\n",
    "        y_true = folder_2_y_true\n",
    "        avg_preds = folder_2_avg_preds\n",
    "    else:\n",
    "        raise Exception(\"Invalid entry\")\n",
    "    \n",
    "    avg_preds.append(avg)\n",
    "    y = 1 if row['label'] == 'FAKE' else 0\n",
    "    y_true.append(y)\n",
    "    \n",
    "print(\"folder 0\", log_loss(folder_0_y_true, folder_0_avg_preds))\n",
    "print(\"folder 1\", log_loss(folder_1_y_true, folder_1_avg_preds))\n",
    "print(\"folder 2\", log_loss(folder_2_y_true, folder_2_avg_preds))\n",
    "\n",
    "all_true = folder_0_y_true + folder_1_y_true + folder_2_y_true\n",
    "all_preds = folder_0_avg_preds + folder_1_avg_preds + folder_2_avg_preds\n",
    "print(\"all\", log_loss(all_true, all_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
